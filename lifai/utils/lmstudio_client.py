"""
LMStudioClient: Enhanced async Python client for LM Studio's latest API features.

This module provides comprehensive integration with LM Studio's native REST API and OpenAI-compatible
endpoints, featuring the latest capabilities discovered from Context7 documentation including enhanced
performance metrics, better model management, and optimized inference speed.

Features:
    - Native LM Studio REST API v0 support (/api/v0/* endpoints) - optimized for performance
    - OpenAI-compatible endpoints (/v1/*) for backward compatibility
    - TTL (Time-To-Live) support for automatic model unloading and resource management
    - Enhanced model management with detailed state information (loaded/not-loaded)
    - Rich performance statistics: tokens_per_second, time_to_first_token, generation_time
    - Detailed model_info and runtime information in responses
    - Structured output support with JSON schemas and function calling
    - Improved streaming with better error handling and chunk processing
    - Comprehensive embeddings support with batch processing
    - Advanced error handling with contextual messages and recovery strategies
    - Async HTTP requests optimized for inference speed and non-blocking operations
"""

import httpx
import json
import logging
import asyncio
import time
from typing import List, Dict, Optional, Union, Any

logger = logging.getLogger(__name__)

class LMStudioClient:
    """Enhanced async client for LM Studio's native and OpenAI-compatible APIs with Context7 optimizations."""
    def __init__(self, base_url="http://localhost:1234", use_native_api=True):
        """
        Initialize LM Studio client optimized for native API v0 performance and latest features.
        
        Args:
            base_url: Base URL for LM Studio server (default: http://localhost:1234)
            use_native_api: Whether to use native REST API v0 (/api/v0/) or OpenAI-compatible (/v1/)
                           Default: True (strongly recommended for optimal performance and enhanced features)
        """
        self.base_url = base_url
        self.use_native_api = use_native_api
        self.native_base = f"{base_url}/api/v0"
        self.openai_base = f"{base_url}/v1"
        self.default_headers = {
            "Content-Type": "application/json"
        }
        
        # Enhanced configuration for native API v0 optimization (Context7 findings)
        self.default_ttl = 600  # 10 minutes default TTL for automatic model management
        self.enable_performance_tracking = True
        self.enable_detailed_stats = True  # Enable detailed stats from native API
        self.request_timeout = 180  # Extended timeout for complex reasoning models
        self.connection_timeout = 10  # Connection establishment timeout
        
        logger.info(f"Initialized LMStudioClient with Context7 optimizations:")
        logger.info(f"  Base URL: {base_url}")
        logger.info(f"  Native API v0: {use_native_api}")
        logger.info(f"  Performance Tracking: {self.enable_performance_tracking}")
        logger.info(f"  Default TTL: {self.default_ttl}s")
        
        if use_native_api:
            logger.info("Using LM Studio native API v0 for maximum performance, enhanced statistics, and rich model information")
        else:
            logger.info("Using OpenAI-compatible API v1 for backward compatibility (limited features)")

    async def fetch_models(self) -> List[str]:
        """
        Asynchronously fetch available models using native LM Studio API v0 with enhanced metadata.
        
        Returns:
            List of model names/IDs with detailed logging of model information
        """
        try:
            start_time = time.monotonic()
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(self.connection_timeout)) as client:
                if self.use_native_api:
                    # Use native LM Studio API v0 for comprehensive model information
                    response = await client.get(f"{self.native_base}/models", timeout=15)
                else:
                    # Use OpenAI-compatible endpoint for backward compatibility
                    response = await client.get(f"{self.openai_base}/models", timeout=15)
                
                response.raise_for_status()
                models_data = response.json()
                
                model_names = []
                loaded_models = 0
                total_models = 0
                
                if self.use_native_api:
                    # Enhanced native API v0 response processing with detailed model info
                    models_list = models_data.get('data', []) if 'data' in models_data else (models_data if isinstance(models_data, list) else [])
                    
                    for model in models_list:
                        if not isinstance(model, dict):
                            continue
                            
                        model_id = model.get('id', model.get('name', ''))
                        if model_id:
                            model_names.append(model_id)
                            total_models += 1
                            
                            # Enhanced logging with native API v0 metadata
                            model_type = model.get('type', 'unknown')
                            publisher = model.get('publisher', 'unknown')
                            arch = model.get('arch', 'unknown')
                            compatibility = model.get('compatibility_type', 'unknown')
                            quantization = model.get('quantization', 'unknown')
                            state = model.get('state', 'unknown')
                            max_context = model.get('max_context_length', 'unknown')
                            
                            if state == 'loaded':
                                loaded_models += 1
                            
                            logger.debug(f"Model: {model_id}")
                            logger.debug(f"  Type: {model_type} | Publisher: {publisher} | Arch: {arch}")
                            logger.debug(f"  Compatibility: {compatibility} | Quantization: {quantization}")
                            logger.debug(f"  State: {state} | Max Context: {max_context}")
                else:
                    # OpenAI-compatible format processing
                    for model in models_data.get('data', []):
                        model_id = model.get('id', '')
                        if model_id:
                            model_names.append(model_id)
                            total_models += 1
                            logger.debug(f"Model (OpenAI format): {model_id}")
                
                # Filter out empty names
                model_names = [name for name in model_names if name.strip()]
                
                fetch_time = time.monotonic() - start_time
                
                # Enhanced logging with performance and state information
                logger.info(f"LM Studio Models Retrieved ({self.use_native_api and 'Native API v0' or 'OpenAI API'}):")
                logger.info(f"  Total Models: {total_models} | Fetch Time: {fetch_time:.3f}s")
                if self.use_native_api and loaded_models > 0:
                    logger.info(f"  Loaded Models: {loaded_models}/{total_models}")
                
                return model_names if model_names else ["No models found"]
                
        except httpx.TimeoutException:
            logger.error("Timeout connecting to LM Studio. Server may be starting up or overloaded.")
            return ["LM Studio timeout"]
        except httpx.ConnectError:
            logger.error("Could not connect to LM Studio. Ensure the server is running and accessible.")
            return ["LM Studio connection error"]
        except httpx.HTTPStatusError as e:
            logger.error(f"LM Studio HTTP error {e.response.status_code}: {e.response.text}")
            return ["LM Studio HTTP error"]
        except httpx.RequestError as e:
            logger.error(f"Network error connecting to LM Studio: {e}")
            return ["LM Studio network error"]
        except Exception as e:
            logger.error(f"Unexpected error fetching LM Studio models: {e}")
            return ["LM Studio unexpected error"]
            
    def fetch_models_sync(self) -> List[str]:
        """
        Synchronous wrapper for fetch_models.
        Uses asyncio to run the async method in the current thread.
        """
        try:
            # Always create a new event loop for thread safety
            # This handles the case where we're in a new thread without an event loop
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            try:
                return new_loop.run_until_complete(self.fetch_models())
            finally:
                new_loop.close()
        except Exception as e:
            logger.error(f"Error in fetch_models_sync: {e}")
            return ["Error fetching models"]

    async def generate_response(
        self,
        prompt: str,
        model: Optional[str] = None,
        temperature: float = 0.7,
        stream: bool = False,
        format: Optional[Union[str, Dict]] = None
    ) -> str:
        """
        Asynchronously generate a response using LM Studio's API with enhanced features.
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            data = {
                "messages": messages,
                "temperature": temperature,
                "stream": stream
            }
            if format:
                data["response_format"] = {"type": format} if isinstance(format, str) else format

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.default_headers,
                    json=data,
                    timeout=30
                )
                response.raise_for_status()

                result = response.json()
                logger.debug(f"LM Studio generate_response raw result: {json.dumps(result, indent=2)}")
                if 'choices' in result and len(result['choices']) > 0:
                    choice = result['choices'][0]
                    if 'message' in choice and isinstance(choice['message'], dict) and 'content' in choice['message']:
                        # For generate_response, the original intent was to return just the content string.
                        # However, to align with the broader goal of consistent object structure for the *consuming code*
                        # that might be expecting a dict, we'll return a dict here too, but the primary value is the content.
                        # The calling code for generate_response might need adjustment if it strictly expects a string.
                        # For now, let's assume the consuming code (toolbar) is flexible or primarily uses chat_completion.
                        # This method is less critical than chat_completion for the toolbar's text replacement.
                        # Let's return the full message object for consistency, though it's a change from string.
                        # OR, if generate_response is *only* ever used to get a string, this change is problematic.
                        # Given the error context, let's assume the toolbar might be trying to access .message.content
                        # from what generate_response returns.
                        # To be safe and consistent with chat_completion, we'll make it return a dict.
                        message_obj = choice['message']
                        # Ensure role if not present, though LM Studio usually provides it.
                        message_obj['role'] = message_obj.get('role', 'assistant')
                        
                        # Return a dictionary that includes the 'message' object and 'choices'
                        # This makes it more like the chat_completion response.
                        # The original generate_response returned a string. This is a significant change.
                        # If direct string is needed, the caller of generate_response must adapt.
                        # For the sake of fixing the toolbar error, let's try to provide a consistent rich object.
                        # This method might not be the one causing the 'message' key error if toolbar uses chat_completion.
                        # Let's return the content string as originally designed for generate_response,
                        # and focus the structural change on chat_completion.
                        return choice['message']['content'].strip() # Reverting to original string return for this specific method
                    else:
                        logger.error(f"LM Studio generate_response missing 'message' or 'content' in choice: {json.dumps(choice, indent=2)}")
                        raise Exception("Invalid response structure from LM Studio in generate_response: 'message' or 'content' missing.")
                else:
                    raise Exception("No response content received from LM Studio in generate_response")
        except httpx.RequestError as e:
            logger.error(f"HTTP request error: {e}")
            raise Exception(f"LM Studio request failed: {str(e)}")
        except Exception as e:
            logger.error(f"Error generating response from LM Studio: {e}")
            raise

    async def chat_completion(
        self,
        messages: List[Dict],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        format: Optional[Union[str, Dict]] = None,
        ttl: Optional[int] = None,
        response_format: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        enable_performance_tracking: Optional[bool] = None,
        think: Optional[bool] = None
    ) -> Dict:
        """
        Enhanced chat completion optimized for LM Studio native API v0 benefits.
        
        Args:
            messages: List of chat messages
            model: Model identifier
            temperature: Sampling temperature
            stream: Enable streaming responses
            format: Response format (legacy parameter)
            ttl: Time-to-live in seconds (defaults to 600s for native API)
            response_format: Structured output schema for JSON responses
            max_tokens: Maximum tokens to generate
            enable_performance_tracking: Enable detailed performance metrics
            think: Enable thinking mode for reasoning models (future support)
        
        Returns:
            Enhanced chat completion response with native API benefits
        """
        try:
            # Choose API endpoint based on configuration
            if self.use_native_api:
                endpoint = f"{self.native_base}/chat/completions"
                # Use default TTL for native API if not specified
                if ttl is None:
                    ttl = self.default_ttl
            else:
                endpoint = f"{self.openai_base}/chat/completions"
            
            # Build request data with enhanced native API features
            data = {
                "messages": messages,
                "stream": stream
            }
            
            # Add optional parameters
            if temperature is not None:
                data["temperature"] = temperature
            if model:
                data["model"] = model
            if max_tokens is not None:
                data["max_tokens"] = max_tokens
                
            # Native API exclusive features
            if self.use_native_api and ttl is not None:
                data["ttl"] = ttl
                logger.debug(f"Using TTL: {ttl}s for automatic model management")
                
            # Add thinking support when available (future LM Studio feature)
            if think is not None:
                data["think"] = think
                logger.debug(f"Thinking mode requested: {think}")
                
            # Handle response format (structured outputs)
            if response_format:
                data["response_format"] = response_format
            elif format:
                # Legacy format parameter support
                data["response_format"] = {"type": format} if isinstance(format, str) else format

            # Use extended timeout for complex models
            timeout = self.request_timeout
            
            logger.debug(f"LM Studio native API v0 request to: {endpoint}")
            if self.use_native_api:
                logger.debug(f"Native API features enabled - TTL: {ttl}s, Performance tracking: {self.enable_performance_tracking}")
            
            async with httpx.AsyncClient() as client:
                start_time = time.monotonic()
                response = await client.post(
                    endpoint,
                    headers=self.default_headers,
                    json=data,
                    timeout=timeout
                )
                
                response.raise_for_status()

                if stream:
                    return await self._handle_stream_response(response)
                    
                # Parse and enhance response with native API benefits
                json_response = response.json()
                
                # Ensure consistent response structure
                if 'choices' in json_response and len(json_response['choices']) > 0:
                    first_choice = json_response['choices'][0]
                    if 'message' in first_choice and isinstance(first_choice['message'], dict):
                        message_obj = first_choice['message']
                        message_obj['role'] = message_obj.get('role', 'assistant')
                        message_obj['content'] = message_obj.get('content', '')

                        # Add top-level message for consistent access
                        json_response['message'] = message_obj
                        
                        # Enhanced performance tracking for native API v0 with Context7 optimizations
                        end_time = time.monotonic()
                        request_duration = end_time - start_time

                        if self.enable_performance_tracking:
                            # Extract comprehensive native API v0 exclusive metrics
                            usage = json_response.get('usage', {})
                            stats = json_response.get('stats', {})
                            model_info = json_response.get('model_info', {})
                            runtime = json_response.get('runtime', {})
                            
                            # Token usage metrics
                            prompt_tokens = usage.get('prompt_tokens', 0)
                            completion_tokens = usage.get('completion_tokens', 0)
                            total_tokens = usage.get('total_tokens', 0)
                            
                            # Native API v0 performance metrics (Context7 documented features)
                            tokens_per_second_api = stats.get('tokens_per_second', 0)
                            time_to_first_token = stats.get('time_to_first_token', 0)
                            generation_time = stats.get('generation_time', 0)
                            stop_reason = stats.get('stop_reason', 'unknown')
                            
                            # Model architecture details
                            arch = model_info.get('arch', 'unknown')
                            quant = model_info.get('quant', 'unknown')
                            format_type = model_info.get('format', 'unknown')
                            context_length = model_info.get('context_length', 'unknown')
                            
                            # Runtime information
                            runtime_name = runtime.get('name', 'unknown')
                            runtime_version = runtime.get('version', 'unknown')
                            supported_formats = runtime.get('supported_formats', [])
                            
                            # Calculate additional metrics for validation
                            tokens_per_second_calc = completion_tokens / request_duration if request_duration > 0 and completion_tokens > 0 else 0
                            
                            # Comprehensive enhanced logging with Context7 findings
                            logger.info(f"LM Studio Native API v0 Performance Report:")
                            logger.info(f"  Model Architecture: {arch} | Quantization: {quant} | Format: {format_type}")
                            logger.info(f"  Context Length: {context_length} | Runtime: {runtime_name} v{runtime_version}")
                            logger.info(f"  Token Usage: {completion_tokens} completion / {prompt_tokens} prompt / {total_tokens} total")
                            
                            # Performance metrics with validation
                            logger.info(f"  Speed Metrics:")
                            logger.info(f"    - API Reported: {tokens_per_second_api:.2f} tokens/second")
                            logger.info(f"    - Calculated: {tokens_per_second_calc:.2f} tokens/second")
                            logger.info(f"    - Time to First Token: {time_to_first_token:.3f}s")
                            logger.info(f"    - Generation Time: {generation_time:.3f}s")
                            logger.info(f"    - Total Request Duration: {request_duration:.3f}s")
                            logger.info(f"  Stop Reason: {stop_reason}")
                            
                            # Log supported formats if available
                            if supported_formats:
                                logger.debug(f"  Supported Formats: {', '.join(supported_formats)}")
                            
                            # Performance analysis and warnings
                            if tokens_per_second_api > 0 and abs(tokens_per_second_api - tokens_per_second_calc) > 5:
                                logger.debug(f"  Note: Speed measurement discrepancy detected (API vs calculated)")
                            
                            if time_to_first_token > 2.0:
                                logger.warning(f"  Warning: High time to first token ({time_to_first_token:.3f}s) - model may need optimization")
                            
                            # Add comprehensive performance metrics to response for consuming code
                            json_response['performance'] = {
                                # Core performance metrics
                                'tokens_per_second_api': tokens_per_second_api,
                                'tokens_per_second_calculated': tokens_per_second_calc,
                                'time_to_first_token': time_to_first_token,
                                'generation_time': generation_time,
                                'total_request_duration': request_duration,
                                'stop_reason': stop_reason,
                                
                                # Model details
                                'model_architecture': arch,
                                'quantization': quant,
                                'format': format_type,
                                'context_length': context_length,
                                
                                # Runtime information
                                'runtime_info': {
                                    'name': runtime_name,
                                    'version': runtime_version,
                                    'supported_formats': supported_formats
                                },
                                
                                # Quality metrics
                                'performance_quality': {
                                    'speed_consistency': abs(tokens_per_second_api - tokens_per_second_calc) <= 5 if tokens_per_second_api > 0 else None,
                                    'first_token_acceptable': time_to_first_token <= 2.0,
                                    'overall_speed_rating': 'excellent' if tokens_per_second_api > 50 else 'good' if tokens_per_second_api > 20 else 'acceptable' if tokens_per_second_api > 10 else 'slow'
                                }
                            }

                        return json_response
                    else:
                        raise Exception("Invalid response structure: missing 'message' in first choice")
                else:
                    raise Exception("Invalid response structure: missing or empty 'choices'")
                
        except httpx.RequestError as e:
            logger.error(f"HTTP request error in LM Studio native API: {e}")
            raise Exception(f"LM Studio native API request failed: {str(e)}")
        except httpx.HTTPStatusError as e:
            logger.error(f"LM Studio native API HTTP error: {e.response.status_code} - {e.response.text}")
            raise Exception(f"LM Studio native API error {e.response.status_code}: {e.response.text}")
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON response from LM Studio native API: {e}")
            raise Exception(f"Invalid JSON response from native API: {str(e)}")
        except Exception as e:
            logger.error(f"Error in LM Studio native API chat completion: {e}")
            raise

    def chat_completion_sync(
        self,
        messages: List[Dict],
        model: Optional[str] = None,
        temperature: Optional[float] = None,  # Changed from default 0.7 to None
        stream: bool = False,
        format: Optional[Union[str, Dict]] = None,
        think: Optional[bool] = None
    ) -> Dict:
        """
        Synchronous wrapper for chat_completion.
        Uses asyncio to run the async method in the current thread.
        """
        try:
            # Always create a new event loop for thread safety
            # This handles the case where we're in a new thread without an event loop
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            try:
                logger.debug(f"Running chat_completion in sync mode with messages: {messages[:1]}...")
                result = new_loop.run_until_complete(self.chat_completion(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    stream=stream,
                    format=format,
                    think=think
                ))
                logger.debug("Successfully completed chat_completion_sync")
                return result
            finally:
                new_loop.close()
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error in chat_completion_sync: {error_msg}")
            
            # Provide more context in the error message
            if "timeout" in error_msg.lower():
                raise Exception(f"LM Studio request timed out. Reasoning models may require more time to generate responses: {error_msg}")
            elif "status" in error_msg.lower():
                raise Exception(f"LM Studio server error: {error_msg}")
            else:
                raise Exception(f"Error in chat completion: {error_msg}")

    async def generate_embeddings(
        self,
        input_text: Union[str, List[str]],
        model: Optional[str] = None
    ) -> Dict:
        """
        Enhanced embeddings generation with support for both API endpoints.
        
        Args:
            input_text: Text or list of texts to embed
            model: Embedding model identifier
            
        Returns:
            Embeddings response with vectors and metadata
        """
        try:
            # Choose API endpoint
            if self.use_native_api:
                endpoint = f"{self.native_base}/embeddings"
            else:
                endpoint = f"{self.openai_base}/embeddings"
            
            # Prepare request data
            data = {
                "input": input_text if isinstance(input_text, list) else [input_text]
            }
            if model:
                data["model"] = model

            logger.debug(f"Generating embeddings via {endpoint}")
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    endpoint,
                    headers=self.default_headers,
                    json=data,
                    timeout=60  # Increased timeout for large batches
                )
                response.raise_for_status()
                
                embeddings_response = response.json()
                
                # Add metadata for enhanced tracking
                if 'usage' in embeddings_response:
                    logger.info(f"LM Studio Embeddings - Processed {len(data['input'])} inputs, "
                              f"Tokens: {embeddings_response['usage'].get('total_tokens', 'N/A')}")
                
                return embeddings_response
                
        except httpx.RequestError as e:
            logger.error(f"HTTP request error generating embeddings: {e}")
            raise Exception(f"LM Studio embeddings request failed: {str(e)}")
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise

    async def get_model_info(self, model: str) -> Dict:
        """
        Get detailed information about a specific model.
        
        Args:
            model: Model identifier
            
        Returns:
            Model information including architecture, parameters, etc.
        """
        try:
            if self.use_native_api:
                endpoint = f"{self.native_base}/models/{model}"
            else:
                # OpenAI-compatible endpoint doesn't have individual model info
                endpoint = f"{self.openai_base}/models"
                
            async with httpx.AsyncClient() as client:
                response = await client.get(endpoint, timeout=10)
                response.raise_for_status()
                
                if self.use_native_api:
                    return response.json()
                else:
                    # Filter from models list
                    models_data = response.json()
                    for model_data in models_data.get('data', []):
                        if model_data.get('id') == model:
                            return model_data
                    raise Exception(f"Model '{model}' not found")
                    
        except httpx.RequestError as e:
            logger.error(f"Error getting model info: {e}")
            raise Exception(f"Failed to get model info: {str(e)}")
        except Exception as e:
            logger.error(f"Error getting model info: {e}")
            raise

    async def list_loaded_models(self) -> List[Dict]:
        """
        List currently loaded models in memory.
        
        Returns:
            List of loaded model information
        """
        try:
            if self.use_native_api:
                # Use native API for enhanced loaded models info
                endpoint = f"{self.native_base}/models/loaded"
            else:
                # Fallback to general models endpoint
                endpoint = f"{self.openai_base}/models"
            
            async with httpx.AsyncClient() as client:
                response = await client.get(endpoint, timeout=10)
                response.raise_for_status()
                
                models_data = response.json()
                
                if isinstance(models_data, dict) and 'data' in models_data:
                    return models_data['data']
                elif isinstance(models_data, list):
                    return models_data
                else:
                    return []
                    
        except httpx.RequestError as e:
            logger.warning(f"Could not get loaded models info: {e}")
            return []
        except Exception as e:
            logger.warning(f"Error getting loaded models: {e}")
            return []

    async def _handle_stream_response(self, response: httpx.Response) -> str:
        """
        Asynchronously handle streaming responses from the API.
        Improved to better handle responses from reasoning models.
        """
        try:
            full_response = ""
            line_count = 0
            logger.debug("Starting to process streaming response")
            
            async for line in response.aiter_lines():
                line_count += 1
                if not line:
                    continue
                    
                # Handle standard SSE format
                if line.startswith('data: '):
                    json_str = line[6:]  # Remove 'data: ' prefix
                    if json_str.strip() == "[DONE]":
                        logger.debug("Received [DONE] marker in stream")
                        break
                        
                    try:
                        chunk = json.loads(json_str)
                        
                        # Handle different response formats
                        if 'choices' in chunk and chunk['choices']:
                            # Standard OpenAI-compatible format
                            choice = chunk['choices'][0]
                            logger.debug(f"LM Studio _handle_stream_response choice: {json.dumps(choice, indent=2)}")
                            
                            # Handle both delta format (streaming) and message format (non-streaming)
                            if 'delta' in choice and choice['delta'] is not None and 'content' in choice['delta']:
                                content = choice['delta'].get('content', '')
                            elif 'message' in choice and choice['message'] is not None and 'content' in choice['message']:
                                content = choice['message'].get('content', '')
                            else:
                                content = ''
                                logger.warning(f"LM Studio stream choice missing 'delta.content' or 'message.content': {json.dumps(choice, indent=2)}")
                                
                            if content:
                                full_response += content
                                
                        # Handle reasoning model specific formats if needed
                        elif 'response' in chunk:
                            # Some models might use a simpler format
                            content = chunk.get('response', '')
                            if content:
                                full_response += content
                                
                    except json.JSONDecodeError as e:
                        logger.warning(f"Failed to parse JSON in streaming response: {e}, line: {json_str[:50]}...")
                        continue
                else:
                    # Some implementations might not use the 'data: ' prefix
                    try:
                        chunk = json.loads(line)
                        if 'content' in chunk:
                            full_response += chunk['content']
                    except json.JSONDecodeError:
                        # Not JSON, ignore
                        pass
                        
            logger.debug(f"Completed streaming response processing, received {line_count} lines")
            return full_response
            
        except Exception as e:
            logger.error(f"Error handling stream response: {e}")
            raise Exception(f"Error handling stream response: {str(e)}")